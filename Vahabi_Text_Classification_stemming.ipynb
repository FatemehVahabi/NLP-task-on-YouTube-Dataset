{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1becfaa",
   "metadata": {},
   "source": [
    "### Author:: \\<Fatemeh Vahabi\\>\n",
    "ID::     \\<4013614052\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53b13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43050dea-6a78-4409-8c21-4af4d3452611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### در قطعه کد بالا کتابخانه های لازم را فراخوانی کردم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b25b1d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d7de28-6832-461c-9086-7cab2fc9da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### در بخش زیر دیتاست را بارگذاری میکنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250c1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = pd.read_table('E:/arshad/term 2/NLP/exercise_zero/dataset/youtube.csv', encoding='utf-8', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2fc7038-cea7-4f74-af5a-d2e79ed81d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### در بخش زیر همانطور که نوشته شده است، جاهایی از جدول که خالی نیستند را نگه می داریم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed50e9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>z13th1q4yzihf1bll23qxzpjeujterydj</td>\n",
       "      <td>Carmen Racasanu</td>\n",
       "      <td>2014-11-14T13:27:52</td>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k</td>\n",
       "      <td>diego mogrovejo</td>\n",
       "      <td>2014-11-14T13:28:08</td>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>z130zd5b3titudkoe04ccbeohojxuzppvbg</td>\n",
       "      <td>BlueYetiPlayz -Call Of Duty and More</td>\n",
       "      <td>2015-05-23T13:04:32</td>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>z12he50arvrkivl5u04cctawgxzkjfsjcc4</td>\n",
       "      <td>Photo Editor</td>\n",
       "      <td>2015-06-05T14:14:48</td>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k</td>\n",
       "      <td>Ray Benich</td>\n",
       "      <td>2015-06-05T18:05:16</td>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID  \\\n",
       "0    LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU   \n",
       "1    LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A   \n",
       "2    LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8   \n",
       "3            z13jhp0bxqncu512g22wvzkasxmvvzjaz04   \n",
       "4            z13fwbwp1oujthgqj04chlngpvzmtt3r3dw   \n",
       "..                                           ...   \n",
       "345            z13th1q4yzihf1bll23qxzpjeujterydj   \n",
       "346        z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k   \n",
       "347          z130zd5b3titudkoe04ccbeohojxuzppvbg   \n",
       "348          z12he50arvrkivl5u04cctawgxzkjfsjcc4   \n",
       "349        z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k   \n",
       "\n",
       "                                   AUTHOR                 DATE  \\\n",
       "0                               Julius NM  2013-11-07T06:20:48   \n",
       "1                             adam riyati  2013-11-07T12:37:15   \n",
       "2                        Evgeny Murashkin  2013-11-08T17:34:21   \n",
       "3                         ElNino Melendez  2013-11-09T08:28:43   \n",
       "4                                  GsMega  2013-11-10T16:05:38   \n",
       "..                                    ...                  ...   \n",
       "345                       Carmen Racasanu  2014-11-14T13:27:52   \n",
       "346                       diego mogrovejo  2014-11-14T13:28:08   \n",
       "347  BlueYetiPlayz -Call Of Duty and More  2015-05-23T13:04:32   \n",
       "348                          Photo Editor  2015-06-05T14:14:48   \n",
       "349                            Ray Benich  2015-06-05T18:05:16   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "0    Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1    Hey guys check out my new channel and our firs...      1  \n",
       "2               just for test I have to say murdev.com      1  \n",
       "3     me shaking my sexy ass on my channel enjoy ^_^ ﻿      1  \n",
       "4              watch?v=vtaRGgvGtWQ   Check this out .﻿      1  \n",
       "..                                                 ...    ...  \n",
       "345  How can this have 2 billion views when there's...      0  \n",
       "346         I don't now why I'm watching this in 2014﻿      0  \n",
       "347  subscribe to me for call of duty vids and give...      1  \n",
       "348  hi guys please my android photo editor downloa...      1  \n",
       "349  The first billion viewed this because they tho...      0  \n",
       "\n",
       "[350 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case of Nan values for the 'CONTENT' column\n",
    "youtube = youtube[youtube['CONTENT'].notna()]\n",
    "\n",
    "youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3c08be-ad95-4aec-9929-db471ba405bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#!pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d60736-a699-41bf-8bdf-c94bfd9ee7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### در بخش پیش پردازش داده ها حروف بزرگ را به کوچک تبدیل کردم. همچنین فضای خالی اضافی را از بین بردم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9d4d2",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a10148b-7e4f-457f-9168-785ec6d9bd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BORJ_I~1\\AppData\\Local\\Temp/ipykernel_25236/110476635.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  youtube['CONTENT'][i]=(youtube['CONTENT'][i]).lower()\n",
      "C:\\Users\\BORJ_I~1\\AppData\\Local\\Temp/ipykernel_25236/110476635.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  youtube['CONTENT'][i] = youtube['CONTENT'][i].strip()\n"
     ]
    }
   ],
   "source": [
    "# Your preprocessing code goes here\n",
    "# (Note: get the contents of the youtube['CONTENT'] column and replace them with your preprocessed results)\n",
    "\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    #print(youtube['CONTENT'][i])\n",
    "    ## turn to lower case\n",
    "    comment=youtube['CONTENT'][i]\n",
    "    youtube['CONTENT'][i]=(youtube['CONTENT'][i]).lower()\n",
    "    ## delete whitespace\n",
    "    youtube['CONTENT'][i] = youtube['CONTENT'][i].strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e108a-8a2c-4752-a6f2-e2a9b63387e7",
   "metadata": {},
   "source": [
    "#### در قطعه کد زیر استاپ‌ورد ها را از بین بردم که در نتیجه نهایی تاثیر خوبی نداشت و زمانی که این بخش را از مرجله پیش پردازش حذف کردم، نتیجه بهتری به دست آمد.لذا این بخش را فقط برای نمایش به صورت کامنت قرار دادم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469e8d6-fb3e-46b5-9c54-3cf4783ac36f",
   "metadata": {},
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    #print(youtube['CONTENT'][i])\n",
    "    #stop_words = set(stopwords.words(‘english’))\n",
    "    #tokens = word_tokenize(comment)\n",
    "    #result = [i for i in tokens if not i in stop_words]\n",
    "    stop_words = stopwords.words('english')\n",
    "    #tokens = word_tokenize(comment)\n",
    "    words = word_tokenize(youtube['CONTENT'][i])\n",
    "    youtube['CONTENT'][i] = [i for i in words if not i in stop_words]\n",
    "    c=''\n",
    "    for j in youtube['CONTENT'][i]:\n",
    "        c+=j\n",
    "        c+=' '\n",
    "    #print(c)\n",
    "    youtube['CONTENT'][i]=c\n",
    "    #print(youtube['CONTENT'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa6889-f2b0-44fe-abda-2fa8c6255261",
   "metadata": {},
   "source": [
    "import string\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    result = youtube['CONTENT'][i].translate(string.maketrans('',''), string.punctuation)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e33df9-d212-463a-9cef-bcacead75230",
   "metadata": {},
   "source": [
    "import re\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    youtube['CONTENT'][i] = re.sub(r'\\d+', '', youtube['CONTENT'][i])\n",
    "    #print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e397f0a9-87f3-461d-bdd3-dc2b2a20cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7f653-276b-42e3-b243-b1c9fd601fe4",
   "metadata": {},
   "source": [
    "#### در بخش زیر ریشه یابی انجام می شود که ابتدا کتابخانه های لازم فراخوانی شده اند و سپس جمله توکنایز میشود و به یک لیست از کلمات تبدیل میشود.سپس بر روی هر کلمه عملیات ریشه یابی انجام می شود و سپس این کلمات به صورت یک رشته در  می ایند."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae3568-5977-43e3-b0cd-c2f2f498c7c3",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c387ea-5314-4828-9a28-e9dc41bac1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BORJ_I~1\\AppData\\Local\\Temp/ipykernel_25236/60859255.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  youtube['CONTENT'][i]=c\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    #print(youtube['CONTENT'][i])\n",
    "    #k = []\n",
    "    temp=word_tokenize(youtube['CONTENT'][i])\n",
    "    k=[]\n",
    "    for word in temp:\n",
    "        k.append(stemmer.stem(word))\n",
    "        #k.append(stemmer.stem(word))\n",
    "    c=''\n",
    "    for j in k:\n",
    "        c+=j\n",
    "        c+=' '\n",
    "    #print(c)\n",
    "    youtube['CONTENT'][i]=c\n",
    "    #print(youtube['CONTENT'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2aa9ea",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5371cf4a-1482-4f16-9b93-1bad90dc0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bd44892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'!': 323, ':': 147, '\\ufeff': 129, 'i': 124, 'the': 119, 'to': 115, ',': 112, '.': 98, 'and': 95, 'http': 95, ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "vocabulary = []\n",
    "  \n",
    "for comment in youtube['CONTENT']:\n",
    "   \n",
    "    words = nltk.tokenize.word_tokenize(comment)\n",
    "    for word in words:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = nltk.FreqDist(vocabulary)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "794a53ae-ef9c-4183-8bca-f0288d488b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19404a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep 700 most common words as features\n",
    "# Note: Do not change this number\n",
    "word_features = [x[0] for x in vocabulary.most_common(700)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d90dd30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By doing this we'll consider every comment as a vector with 700 elements\n",
    "# So each elements says that it does contain the feature word or not\n",
    "data = []\n",
    "for comment in youtube['CONTENT']:\n",
    "    vector = {}\n",
    "    words = nltk.tokenize.word_tokenize(comment)\n",
    "    for word in word_features:\n",
    "        vector[word] = True if word in words else False\n",
    "        \n",
    "    data.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60a6b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(zip(data, youtube['CLASS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbdb853",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e7025",
   "metadata": {},
   "source": [
    "Note: Check the accuracy value of this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4785137",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train, XY_test = train_test_split(dataset, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "347a3306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model Accuracy: 0.8522727272727273\n"
     ]
    }
   ],
   "source": [
    "nltk_model = SklearnClassifier(KNeighborsClassifier())\n",
    "nltk_model.train(XY_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, XY_test)\n",
    "print(\"model Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3d3e0-578b-48bc-adc6-0479c07f6739",
   "metadata": {},
   "source": [
    "#### با دو عملیات پیش پردازش به این دقت رسیده ایم. اگر عملیات حذف استاپ ورد ها را کامنت نکنیم نتیجه بدون تغییر خواهد بود یعنی 0.7840909090909091!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c166a",
   "metadata": {},
   "source": [
    "### Did you know that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9ae4f",
   "metadata": {},
   "source": [
    "In this example we've constructed our vectors as dictionaries<br>\n",
    "because it's the required format of SklearnClassifier from nltk package<br>\n",
    "but we could consider each sentence (or in our case youtube comment) as<br>\n",
    "a simple 0, 1 array, where each element of this array represents the presence<br>\n",
    "of the corresponding feature word.<br>\n",
    "By doing so, we could use the official scikit package and pass our vectors to <br>\n",
    "the its 'fit' method<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c09f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for comment in youtube['CONTENT']:\n",
    "#     vector = []\n",
    "#     words = nltk.tokenize.word_tokenize(comment)\n",
    "#     for word in word_features:\n",
    "#         vector.append(True if word in words else False)\n",
    "        \n",
    "#     data.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f6f1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(data, youtube['CLASS'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbe58bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "# classifier.fit(X_train, y_train)\n",
    "# predicted = classifier.predict(X_test)\n",
    "\n",
    "# accuracy_score(y_test, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
