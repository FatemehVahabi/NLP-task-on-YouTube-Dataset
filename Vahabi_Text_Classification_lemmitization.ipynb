{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1becfaa",
   "metadata": {},
   "source": [
    "### Author:: \\<Fatemeh Vahabi\\>\n",
    "ID::     \\<4013614052\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53b13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e33970-b6aa-4bf9-976c-e3dc27a7331d",
   "metadata": {},
   "source": [
    "#### در قطعه کد بالا کتابخانه های لازم را فراخوانی کردم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b25b1d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fdd59-4d73-4d93-8dad-60486aaf5bd2",
   "metadata": {},
   "source": [
    "#### در بخش زیر دیتاست را بارگذاری میکنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250c1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = pd.read_table('E:/arshad/term 2/NLP/exercise_zero/dataset/youtube.csv', encoding='utf-8', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1561ecc-979a-465b-8f11-d595aa8cc538",
   "metadata": {},
   "source": [
    "#### در بخش زیر همانطور که نوشته شده است، جاهایی از جدول که خالی نیستند را نگه می داریم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed50e9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>z13th1q4yzihf1bll23qxzpjeujterydj</td>\n",
       "      <td>Carmen Racasanu</td>\n",
       "      <td>2014-11-14T13:27:52</td>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k</td>\n",
       "      <td>diego mogrovejo</td>\n",
       "      <td>2014-11-14T13:28:08</td>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>z130zd5b3titudkoe04ccbeohojxuzppvbg</td>\n",
       "      <td>BlueYetiPlayz -Call Of Duty and More</td>\n",
       "      <td>2015-05-23T13:04:32</td>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>z12he50arvrkivl5u04cctawgxzkjfsjcc4</td>\n",
       "      <td>Photo Editor</td>\n",
       "      <td>2015-06-05T14:14:48</td>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k</td>\n",
       "      <td>Ray Benich</td>\n",
       "      <td>2015-06-05T18:05:16</td>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID  \\\n",
       "0    LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU   \n",
       "1    LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A   \n",
       "2    LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8   \n",
       "3            z13jhp0bxqncu512g22wvzkasxmvvzjaz04   \n",
       "4            z13fwbwp1oujthgqj04chlngpvzmtt3r3dw   \n",
       "..                                           ...   \n",
       "345            z13th1q4yzihf1bll23qxzpjeujterydj   \n",
       "346        z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k   \n",
       "347          z130zd5b3titudkoe04ccbeohojxuzppvbg   \n",
       "348          z12he50arvrkivl5u04cctawgxzkjfsjcc4   \n",
       "349        z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k   \n",
       "\n",
       "                                   AUTHOR                 DATE  \\\n",
       "0                               Julius NM  2013-11-07T06:20:48   \n",
       "1                             adam riyati  2013-11-07T12:37:15   \n",
       "2                        Evgeny Murashkin  2013-11-08T17:34:21   \n",
       "3                         ElNino Melendez  2013-11-09T08:28:43   \n",
       "4                                  GsMega  2013-11-10T16:05:38   \n",
       "..                                    ...                  ...   \n",
       "345                       Carmen Racasanu  2014-11-14T13:27:52   \n",
       "346                       diego mogrovejo  2014-11-14T13:28:08   \n",
       "347  BlueYetiPlayz -Call Of Duty and More  2015-05-23T13:04:32   \n",
       "348                          Photo Editor  2015-06-05T14:14:48   \n",
       "349                            Ray Benich  2015-06-05T18:05:16   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "0    Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1    Hey guys check out my new channel and our firs...      1  \n",
       "2               just for test I have to say murdev.com      1  \n",
       "3     me shaking my sexy ass on my channel enjoy ^_^ ﻿      1  \n",
       "4              watch?v=vtaRGgvGtWQ   Check this out .﻿      1  \n",
       "..                                                 ...    ...  \n",
       "345  How can this have 2 billion views when there's...      0  \n",
       "346         I don't now why I'm watching this in 2014﻿      0  \n",
       "347  subscribe to me for call of duty vids and give...      1  \n",
       "348  hi guys please my android photo editor downloa...      1  \n",
       "349  The first billion viewed this because they tho...      0  \n",
       "\n",
       "[350 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case of Nan values for the 'CONTENT' column\n",
    "youtube = youtube[youtube['CONTENT'].notna()]\n",
    "\n",
    "youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3c08be-ad95-4aec-9929-db471ba405bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#!pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e0ceb3b-6677-4a39-9976-515efbdff469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### در بخش پیش پردازش داده ها حروف بزرگ را به کوچک تبدیل کردم. همچنین فضای خالی اضافی را از بین بردم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9d4d2",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a10148b-7e4f-457f-9168-785ec6d9bd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BORJ_I~1\\AppData\\Local\\Temp/ipykernel_2348/110476635.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  youtube['CONTENT'][i]=(youtube['CONTENT'][i]).lower()\n",
      "C:\\Users\\BORJ_I~1\\AppData\\Local\\Temp/ipykernel_2348/110476635.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  youtube['CONTENT'][i] = youtube['CONTENT'][i].strip()\n"
     ]
    }
   ],
   "source": [
    "# Your preprocessing code goes here\n",
    "# (Note: get the contents of the youtube['CONTENT'] column and replace them with your preprocessed results)\n",
    "\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    #print(youtube['CONTENT'][i])\n",
    "    ## turn to lower case\n",
    "    comment=youtube['CONTENT'][i]\n",
    "    youtube['CONTENT'][i]=(youtube['CONTENT'][i]).lower()\n",
    "    ## delete whitespace\n",
    "    youtube['CONTENT'][i] = youtube['CONTENT'][i].strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe4423-ecae-4d8f-88d0-00abe82e5499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b1b6d33-1d95-43e0-8eb2-94524044425b",
   "metadata": {},
   "source": [
    "#### در قطعه کد زیر استاپ‌ورد ها را از بین بردم که در نتیجه نهایی تاثیر خوبی نداشت و زمانی که این بخش را از مرجله پیش پردازش حذف کردم، نتیجه بهتری به دست آمد.لذا این بخش را فقط برای نمایش به صورت کامنت قرار دادم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddcd96-03b0-4041-aa90-05ea22ae7d92",
   "metadata": {},
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(youtube['CONTENT'][i])\n",
    "    youtube['CONTENT'][i] = [i for i in words if not i in stop_words]\n",
    "    c=''\n",
    "    for j in youtube['CONTENT'][i]:\n",
    "        c+=j\n",
    "        c+=' '\n",
    "    \n",
    "    youtube['CONTENT'][i]=c\n",
    "    #print(youtube['CONTENT'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1b7d1-022a-4e79-a420-0e6736fae35c",
   "metadata": {},
   "source": [
    "import string\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    result = youtube['CONTENT'][i].translate(string.maketrans('',''), string.punctuation)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4b180-5ba8-4888-9f48-40154e3b95f4",
   "metadata": {},
   "source": [
    "import re\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    youtube['CONTENT'][i] = re.sub(r'\\d+', '', youtube['CONTENT'][i])\n",
    "    #print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e397f0a9-87f3-461d-bdd3-dc2b2a20cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf77086-5b66-4dc7-9ab5-f0885c59d92a",
   "metadata": {},
   "source": [
    "#### در بخش زیر مرحله لم سازی را میبینید که ابتدا کلمات را توکنایز کردم و یک بردار از کلمات یک جمله ایجاد شده است.در ادامه بر روی هر کلمه عملیات لم سازی را اعمال کردم. و سپس کلمات را مجددا به صورت یک جمله و رشته قبلی در اوردم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552b23c-0848-4f7c-a6e1-27d19da8b85d",
   "metadata": {},
   "source": [
    "### lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeeb4302-19b9-4b2d-86fa-ade5628e7181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BORJ_I~1\\AppData\\Local\\Temp/ipykernel_2348/23600625.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  youtube['CONTENT'][i]=c\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "for i in range (len(youtube['CONTENT'])):\n",
    "    #print(youtube['CONTENT'][i])\n",
    "    temp=word_tokenize(youtube['CONTENT'][i])\n",
    "    k=[]\n",
    "    for word in temp:\n",
    "        k.append(lemmatizer.lemmatize(word))\n",
    "    c=''\n",
    "    for j in k:\n",
    "        c+=j\n",
    "        c+=' '\n",
    "    #print(c)\n",
    "    youtube['CONTENT'][i]=c\n",
    "    #print(youtube['CONTENT'][i])\n",
    "#print(youtube['CONTENT'][269])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719de4ae-6dde-4eb0-8fb6-796e0b433630",
   "metadata": {},
   "source": [
    "#### کدهایی که در ادامه مشاهده میکنید، کدهای اماده ایست که در فایل سوال موجود بود. لذا نیازی به توضیح اضافی نیست. اما در بخش نتیجه نتیجه توضیح داده شده است."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2aa9ea",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5371cf4a-1482-4f16-9b93-1bad90dc0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bd44892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'!': 323, ':': 147, '\\ufeff': 129, 'i': 124, 'the': 119, 'to': 115, ',': 112, '.': 98, 'and': 95, 'http': 95, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "vocabulary = []\n",
    "  \n",
    "for comment in youtube['CONTENT']:\n",
    "    \n",
    "    words = nltk.tokenize.word_tokenize(comment)\n",
    "    for word in words:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = nltk.FreqDist(vocabulary)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "794a53ae-ef9c-4183-8bca-f0288d488b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19404a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep 700 most common words as features\n",
    "# Note: Do not change this number\n",
    "word_features = [x[0] for x in vocabulary.most_common(700)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d90dd30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By doing this we'll consider every comment as a vector with 700 elements\n",
    "# So each elements says that it does contain the feature word or not\n",
    "data = []\n",
    "for comment in youtube['CONTENT']:\n",
    "    vector = {}\n",
    "    words = nltk.tokenize.word_tokenize(comment)\n",
    "    for word in word_features:\n",
    "        vector[word] = True if word in words else False\n",
    "        \n",
    "    data.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60a6b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(zip(data, youtube['CLASS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbdb853",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e7025",
   "metadata": {},
   "source": [
    "Note: Check the accuracy value of this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4785137",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train, XY_test = train_test_split(dataset, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "347a3306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model Accuracy: 0.8863636363636364\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nltk_model = SklearnClassifier(KNeighborsClassifier())\n",
    "nltk_model.train(XY_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, XY_test)\n",
    "print(\"model Accuracy: {}\".format(accuracy))\n",
    "print('____________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ccdd7-f506-48a9-8583-ee792769d065",
   "metadata": {},
   "source": [
    "#### همانطور که در بالا مشاهده میکنید، 0.1 بهبود حاصل شده است. اگر استاپ ورد ها را حذف کنیم نتیجه0.8068181818181818 خواهد بود. به همین علت کد بخش حذف استاپ ورد ها را کامنت کردم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c166a",
   "metadata": {},
   "source": [
    "### Did you know that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9ae4f",
   "metadata": {},
   "source": [
    "In this example we've constructed our vectors as dictionaries<br>\n",
    "because it's the required format of SklearnClassifier from nltk package<br>\n",
    "but we could consider each sentence (or in our case youtube comment) as<br>\n",
    "a simple 0, 1 array, where each element of this array represents the presence<br>\n",
    "of the corresponding feature word.<br>\n",
    "By doing so, we could use the official scikit package and pass our vectors to <br>\n",
    "the its 'fit' method<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c09f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for comment in youtube['CONTENT']:\n",
    "#     vector = []\n",
    "#     words = nltk.tokenize.word_tokenize(comment)\n",
    "#     for word in word_features:\n",
    "#         vector.append(True if word in words else False)\n",
    "        \n",
    "#     data.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f6f1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(data, youtube['CLASS'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe58bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "# classifier.fit(X_train, y_train)\n",
    "# predicted = classifier.predict(X_test)\n",
    "\n",
    "# accuracy_score(y_test, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
